{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: pylab import has clobbered these variables: ['display']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Jennifer Sleeman \n",
    "#DC Data Science Meetup\n",
    "#Example of DCGAN in Keras\n",
    "#Based on code from the following sources:\n",
    "#https://github.com/jacobgil/keras-dcgan/,https://github.com/rajathkumarmp/DCGAN/\n",
    "import pylab as pl\n",
    "from IPython import display\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Reshape\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.convolutional import UpSampling2D\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.core import Flatten\n",
    "from keras.optimizers import SGD,Adam\n",
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "import argparse\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getMNIST():\n",
    "    #Get the data\n",
    "    (X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "    #normalize\n",
    "    X_train = (X_train.astype(np.float32) - 127.5)/127.5\n",
    "    #Reshape\n",
    "    X_train = X_train.reshape((X_train.shape[0], 1) + X_train.shape[1:])\n",
    "    return (X_train,y_train,X_test,y_test)\n",
    "\n",
    "#plot the loss\n",
    "def plot_loss(dloss,gloss):\n",
    "    from IPython import display\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(plt.gcf())\n",
    "    plot(dloss[0:],label='discriminitive loss')\n",
    "    plot(gloss, label='generative loss')\n",
    "    legend()\n",
    "    show()\n",
    "    \n",
    "def combine_images(generated_images):\n",
    "    num = generated_images.shape[0]\n",
    "    width = int(np.sqrt(num))\n",
    "    height = int(np.ceil(float(num)/width))\n",
    "    shape = generated_images.shape[2:]\n",
    "    image = np.zeros((height*shape[0], width*shape[1]), dtype=generated_images.dtype)\n",
    "    for index, img in enumerate(generated_images):\n",
    "        i = int(index/width)\n",
    "        j = index % width\n",
    "        image[i*shape[0]:(i+1)*shape[0], j*shape[1]:(j+1)*shape[1]] = img[0, :, :]\n",
    "    return image\n",
    "\n",
    "#Save the generated images to the file system\n",
    "def save_generated(generated_dir, epoch, index,generated_images):\n",
    "    for index2,img in enumerate(generated_images):\n",
    "        image = combine_images(generated_images)\n",
    "        #image=img[0,:,:]\n",
    "        image=image*127.5+127.5\n",
    "    from PIL import Image\n",
    "    Image.fromarray((image).astype(np.uint8)).save(generated_dir+str(epoch)+\"_\"+str(index)+\".png\")\n",
    "    \n",
    "#Assign random noise\n",
    "#Draw samples from a uniform distribution\n",
    "def assign_random_noise(batch_size, noise):\n",
    "       for i in range(batch_size):\n",
    "            noise[i, :] = np.random.uniform(-1, 1, 100)\n",
    "            \n",
    "#Save the models for later\n",
    "def save_models(discriminator,generator):            \n",
    "    generator.save_weights('generator', True)\n",
    "    discriminator.save_weights('discriminator', True)   \n",
    "    \n",
    "#Print the losses, plot the losses and print one the images\n",
    "def print_stats(g_loss,d_loss,generated_dir,epoch,index):\n",
    "    #save the weights periodically\n",
    "    plot_loss(d_loss, g_loss)\n",
    "    from IPython.display import Image,display\n",
    "    display(Image(filename=generated_dir+str(epoch)+\"_\"+str(index)+\".png\"))\n",
    "    print(\"batch \"+ str(index) +\" d_loss :\"+  str(d_loss[len(d_loss)-1]))\n",
    "    print(\"batch \"+ str(index) +\" g_loss : \"+ str(g_loss[len(g_loss)-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create the generator model\n",
    "def generator_model():\n",
    "    #linear stack of layers model\n",
    "    generator = Sequential()\n",
    "    #take in 100 random inputs\n",
    "    generator.add(Dense( input_dim=100, output_dim=(128*7*7)))\n",
    "    #ramp function 0,inf\n",
    "    generator.add(Activation('relu'))\n",
    "    #reshaping to 128, 7,7\n",
    "    generator.add(Reshape((128, 7, 7)))\n",
    "    #upsampling repeat rows and columns by 2,2\n",
    "    generator.add(UpSampling2D(size=(2, 2)))\n",
    "    #128, 14,14\n",
    "    generator.add(Convolution2D(64, 5, 5, border_mode='same'))\n",
    "    #ramp function 0,inf\n",
    "    generator.add(Activation('relu'))\n",
    "    #upsampling repeat rows and columns by 2,2\n",
    "    generator.add(UpSampling2D(size=(2, 2)))\n",
    "    #128,28,28\n",
    "    generator.add(Convolution2D(1, 5, 5, border_mode='same'))\n",
    "    #tanh -1,1\n",
    "    generator.add(Activation('tanh'))  \n",
    "    return generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create the discriminative model\n",
    "def discriminator_model():\n",
    "    #linear stack of layers model\n",
    "    discriminator = Sequential()\n",
    "    #Convolutional layer with 64 filters and 5 rows 5 columns in kernel, strides 2,2 input shape 1,28,28 for mnist\n",
    "    discriminator.add(Convolution2D(64, 5, 5, border_mode='same', \n",
    "                                    subsample=(2,2), input_shape=(1,28,28)))\n",
    "    #LeakyReLu activation\n",
    "    discriminator.add(LeakyReLU(0.2))\n",
    "    #Convolutional layer with 128 filters and 5 rows 5 columns in kernel, strides 2,2\n",
    "    discriminator.add(Convolution2D(128, 5, 5, border_mode='same', subsample=(2,2)))\n",
    "    #LeakyReLu activation\n",
    "    discriminator.add(LeakyReLU(0.2))\n",
    "    #Flattens\n",
    "    discriminator.add(Flatten())\n",
    "    #Output of dim 1\n",
    "    discriminator.add(Dense(1))\n",
    "    #Activation of Sigmoid [0,1]\n",
    "    discriminator.add(Activation('sigmoid'))\n",
    "    return discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#We do this so the weights from the discriminator can be used by the generator\n",
    "def generator_discriminator(generator, discriminator):\n",
    "    #linear stack of layers model\n",
    "    model = Sequential()\n",
    "    #add the generator\n",
    "    model.add(generator)\n",
    "    #tell the discriminator freeze its weights\n",
    "    discriminator.trainable = False\n",
    "    #add the discriminator\n",
    "    model.add(discriminator)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compile_models(batch_size,learning_rate):\n",
    "    #define our discriminator\n",
    "    discriminator = discriminator_model()\n",
    "    #define our generator\n",
    "    generator = generator_model()\n",
    "\n",
    "    discriminator_generator = \\\n",
    "    generator_discriminator(generator, discriminator)\n",
    "\n",
    "    #set up our parameters (learning rate etc.)\n",
    "    d_optim = Adam(lr=learning_rate, beta_1=0.5, decay=0.001)\n",
    "    g_optim = Adam(lr=learning_rate, beta_1=0.5, decay=0.001)\n",
    "\n",
    "    #compile the generator\n",
    "    generator.compile(loss='binary_crossentropy', optimizer=g_optim, metrics=['accuracy'])\n",
    "\n",
    "    #compile the discriminator/generator\n",
    "    discriminator_generator.compile(\n",
    "        loss='binary_crossentropy', optimizer=g_optim)\n",
    "    #compile the discriminator\n",
    "    discriminator.trainable = True\n",
    "    discriminator.compile(loss='binary_crossentropy', optimizer=d_optim, metrics=['accuracy'])\n",
    "    return (discriminator,generator,discriminator_generator)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(learning_rate,batch_size,num_examples,num_batches,X_train,Y_train,X_test,Y_test,generated_dir):\n",
    "    #loop by epoch and go through the training process\n",
    "    dLoss=[]\n",
    "    gLoss=[]\n",
    "    #compile models\n",
    "    discriminator, generator,discriminator_generator=compile_models(batch_size,learning_rate)\n",
    "    #setting up the noise # 100 is the contributing to the bottleneck\n",
    "    noise = np.zeros((batch_size, 100))\n",
    "    for epoch in range(100):\n",
    "        #from 0 to number of batches\n",
    "        for index in range(num_batches):\n",
    "            #assigning random noise\n",
    "            assign_random_noise(batch_size, noise)\n",
    "\n",
    "            #get a batch of images\n",
    "            image_batch = X_train[index*batch_size:(index+1)*batch_size]\n",
    "\n",
    "            #get our generated images\n",
    "            generated_images = generator.predict(noise, verbose=0)\n",
    "\n",
    "            #periodically save the generated images       \n",
    "            if index % 30 == 0 or index==0:\n",
    "                save_generated(generated_dir, epoch, index, generated_images)\n",
    "\n",
    "            #combine the real and fake images \n",
    "            X = np.concatenate((image_batch, generated_images))\n",
    "            y = [1] * batch_size + [0] * batch_size\n",
    "\n",
    "\n",
    "            #give the real and fake images to the discriminator\n",
    "            #Single gradient update over one batch of samples\n",
    "            d_loss = discriminator.train_on_batch(X, y)\n",
    "            dLoss.append(d_loss[0])\n",
    "\n",
    "\n",
    "            #assigning random noise\n",
    "            assign_random_noise(batch_size, noise)\n",
    "            discriminator.trainable = False\n",
    "\n",
    "            #train combined model\n",
    "            #NOTICE THE LABELS GOING IN ARE ALL 1\n",
    "            g_loss = discriminator_generator.train_on_batch(noise, [1] * batch_size)\n",
    "            gLoss.append(g_loss)\n",
    "            \n",
    "            #unfreeze \n",
    "            discriminator.trainable = True\n",
    "            #save models\n",
    "            if index % 10 == 9 or index==0: \n",
    "                save_models(discriminator,generator)\n",
    "                \n",
    "            #print stats\n",
    "            if index % 30 == 0:\n",
    "                print_stats(gLoss,dLoss,generated_dir,epoch,index)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#initialize parameter\n",
    "#batch_size 128\n",
    "batch_size = 128\n",
    "#learning rate\n",
    "learning_rate = 0.0002\n",
    "#number of epochs\n",
    "nepoch = 50\n",
    "\n",
    "#get the MNIST data\n",
    "X_train,Y_train,X_test,Y_test=getMNIST()\n",
    "#number of examples\n",
    "num_examples = (X_train.shape)[0]\n",
    "#number of batches\n",
    "num_batches = int(num_examples/float(batch_size))\n",
    "\n",
    "print('Number of examples: ',  num_examples)\n",
    "print('Number of batches: ', num_batches)\n",
    "print('Number of epochs: ', nepoch)\n",
    "\n",
    "generated_dir=\"/home/jsleeman/dev/learn/gans/generated/mnist/\"\n",
    "#generated_dir=\"/home/ubuntu/generated/mnist/\"\n",
    "train(learning_rate,batch_size,num_examples,num_batches,X_train,Y_train,X_test,Y_test,generated_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
